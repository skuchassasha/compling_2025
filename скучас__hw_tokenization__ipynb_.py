# -*- coding: utf-8 -*-
"""Скучас "hw_tokenization_.ipynb"

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1n8Q6PeMDWtPnkyCYHijJxxDgP6wH18Gf

# Домашнее задание: Токенизация текста

Дан список текстов, которые нужно токенизировать разными способами
"""

text = [
"The quick brown fox jumps over the lazy dog. It's a beautiful day!",
"Dr. Smith arrived at 5:30 p.m. from New York. The meeting cost $1,000.50.",
"I can't believe she's going! Let's meet at Jane's house. They'll love it.",
"What's the ETA for the package? Please e-mail support@example.com ASAP!"
]

"""Используйте способы токенизации, которые описаны в практикуме. Каждый способ нужно обернуть в функцию, например:

 ```python
 def simple_tokenization(string):
   return string.split()
   ```

1. Напишите функцию для токенизации по пробелам и знакам препинания (используйте оператор `def`)
"""

def TOKpunct(string):
  import re
  string = re.findall(r"\w+", string)
  return string
for i in text:
  print(TOKpunct(i))

"""2. Напишите функцию для токенизации текста с помощью NLTK"""

def TOKnltk(string):
  import nltk
  nltk.download('punkt')
  nltk.download('punkt_tab')

  from nltk.tokenize import word_tokenize

  return(word_tokenize(string))

for i in text:
  print(TOKnltk(i))

"""3. Напишите функцию для токенизации текста с помощью Spacy"""

def TOKspacy(string):
  import spacy

  nlp = spacy.load("en_core_web_sm")
  doc = nlp(string)

  return([t.text for t in doc])
for i in text:
  print(TOKspacy(i))

"""4. С помощью цикла `for` примените каждую из написанных функций к каждому тексту из списка `texts`"""

for i in text:
  print("Tokenization 1")
  print(TOKpunct(i))
  print("Tokenization 2")
  print(TOKnltk(i))
  print("Tokenization 3")
  print(TOKspacy(i))
  print()

"""##### Критерии оценки (макс. балл == 5):

- Функциональность (до 4 баллов)): Все методы работают корректно (запускаем код, и он работает)
- Качество кода (до 1 балла): Чистый, документированный код с обработкой ошибок (кратко описать, что вы дополнили самостоятельно, например, "добавлена токенизация `spacy`")

Теоретические вопросы (макс. балл == 5; в ведомость выставляется сумма за практику и теорию)

Необходимо дать краткие ответы на вопросы по теме "токенизация". В сумме длина ответов на вопрос не должна превышать размер вордовской страницы 14 шрифтом.

1. Почему простое разделение текста по пробелам и знакам препинания часто является недостаточным для современных NLP-задач? Приведите 2-3 конкретных примера, когда деление текста по символам или словам не работает. (2 балла за полный и подробный ответ на вопрос).
Ответ: В первую очередь в голову приходят задачи, в рамках которых исзучаются н-граммы, то есть интерес представляют не отдельные слова, а встречаемость слов вместе. К тому же, как упоминалось в практикуме, бывают случаи, когда слово является Out-of-Vocabulary и необходима также посимвольная токенизация. Разделение по пробелам тоже может быть бессмысленным, если мы имеем дело, например, с китайским языком, где пробелы зачастую не используются.

2. Сколько токенов во фразе "You shall know a word by the company it keeps" в модели GPT-5? Как вы получили это значение? (1 балл за правильный ответ и ссылку на ресурс, с помощью которого вы узнали эту информацию).
Ответ: Согласно https://gpt-tokenizer.dev/, в этой фразе 10 токенов.

3. Опишите своими словами работу алгоритма BPE (можно форматировать ответ с использованием списков, 2 балла за корректное описание и ясное изложение ответа).
Ответ: Алгоритм ВРЕ выделяет в один (суб)токен часто встречающиеся сочетания символов (грубо говоря, aaBabCaa преобразуется в XBabCX) на основе загруженного массива слов. Таким образом, можно сказать, что этот алгоритм выделяет в качестве субтокенов подслова (словообразующие и формообразующие части слов, характерные для языка), благодаря чему, например, в теории должна более эффективно происходить токенизация разных форм одного и того же слова.
"""